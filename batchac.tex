\begin{algorithm}[t!]
\caption{Batch Actor-Critic Algorithm}
\begin{algorithmic}[1]
\label{alg:batchac}
\REQUIRE Base policy $\pi_\theta(a_t|s_t)$

\WHILE{true}
    \STATE Sample $\{s_i,a_i\}$ from $\pi_\theta(a|s)$ (run it on a robot)
    \STATE Fit $\hat{V}_\phi(s)$ to sampled reward sums
    \STATE Evaluate $\hat{A}^\pi(s_i,a_i) = r(s_i,a_i)+\hat{V}_\phi(s'_i)-\hat{V}_\phi(s_i)$
    \STATE $\nabla_\theta J(\theta) \simeq \sum_i\nabla_\theta\log \pi_\theta(a_i|s_i)\hat{A}^\pi(s_i,a_i)$
    \STATE Improve policy by $theta \leftarrow \theta + \alpha\nabla_\theta J(\theta)$
\ENDWHILE
\RETURN optimal policy from gradient ascent as $\pi^{return}$
\end{algorithmic}
\end{algorithm}